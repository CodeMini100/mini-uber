name: Python Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 2  # Fetch at least two commits for diff
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Install test requirements with more tolerance for version issues
        if [ -f tests/requirements.txt ]; then
          echo "Installing test requirements with version flexibility..."
          # Create a modified requirements file without exact versions
          grep -v "^#" tests/requirements.txt | sed 's/==.*$//' > tests/flexible_requirements.txt
          pip install -r tests/flexible_requirements.txt
        fi
        
        # Install main requirements
        if [ -f requirements.txt ]; then
          echo "Installing main requirements..."
          pip install -r requirements.txt
        fi
        
        # Ensure test packages are installed
        echo "Installing test frameworks..."
        pip install pytest pytest-cov httpx fastapi
        
    - name: Run tests and capture results
      id: test-run
      continue-on-error: true
      run: |
        mkdir -p test_artifacts
        python -m pytest -v > test_artifacts/test_results.txt 2>&1
        echo "status=$?" >> $GITHUB_ENV

    - name: Generate coverage report
      run: |
        pytest --cov=./ --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

    - name: Collect repository files and analyze if tests failed
      if: env.status != '0'
      run: |
        mkdir -p test_artifacts/repo_files
        
        # Create a function to collect Python files
        collect_dir_files() {
          local dir=$1
          
          # Skip if directory doesn't exist
          if [ ! -d "$dir" ]; then
            echo "Directory $dir does not exist, skipping"
            return
          fi
          
          echo "Collecting files from $dir"
          find "$dir" -name "*.py" -type f -not -path "*/\.*" -not -path "*/__pycache__/*" | while read file; do
            dest_file="test_artifacts/repo_files/$file"
            mkdir -p "$(dirname "$dest_file")"
            cp "$file" "$dest_file"
          done
        }
        
        # Collect files from each directory
        collect_dir_files "payments"
        collect_dir_files "ratings"
        collect_dir_files "riders"
        collect_dir_files "rides"
        collect_dir_files "drivers"
        
        # Copy individual files
        cp main.py test_artifacts/repo_files/main.py
        cp config.py test_artifacts/repo_files/config.py
        
        # Encode test results as base64 to preserve formatting
        TEST_RESULTS_BASE64=$(cat test_artifacts/test_results.txt | base64 -w 0)
        
        # Create initial JSON payload with test results only
        cat > test_artifacts/payload.json << EOF
        {
          "test_results_base64": "${TEST_RESULTS_BASE64}",
          "repo_files": {}
        }
        EOF
        
        # Add all repository files to payload
        echo "Adding repository files to payload"
        jq '.repo_files = {}' test_artifacts/payload.json > test_artifacts/payload_with_files.json
        
        find test_artifacts/repo_files -type f -name "*.py" | while read file; do
          # Get relative path
          rel_path=$(echo "$file" | sed 's|test_artifacts/repo_files/||')
          
          # Add file content to the payload
          FILE_CONTENT=$(cat "$file" | jq -Rs .)
          jq --arg file "$rel_path" --arg content "$FILE_CONTENT" '.repo_files[$file] = $content' test_artifacts/payload_with_files.json > test_artifacts/payload_tmp.json
          mv test_artifacts/payload_tmp.json test_artifacts/payload_with_files.json
        done
        
        mv test_artifacts/payload_with_files.json test_artifacts/payload.json
        
        # Send to Vercel function and store results
        echo "Sending test results to AI analyzer..."
        curl -X POST https://uber-feedback-agent.vercel.app/api/analyze \
          -H "Content-Type: application/json" \
          -d @test_artifacts/payload.json \
          > test_artifacts/ai_feedback.json
        
        # Extract feedback from response
        cat test_artifacts/ai_feedback.json | jq -r '.feedback' > test_artifacts/ai_feedback.md
        
        # Display feedback in the log
        echo "AI Analysis of Test Failures:"
        cat test_artifacts/ai_feedback.md
        
        # Save feedback as a GitHub step output
        echo "::set-output name=feedback::$(cat test_artifacts/ai_feedback.md | sed 's/\n/%0A/g' | sed 's/\r/%0D/g')"
    
    - name: Comment on PR with feedback 
      if: env.status != '0' && github.event_name == 'pull_request'
      run: |
        # Read the feedback from file
        FEEDBACK=$(cat test_artifacts/ai_feedback.md)
        
        # Use GitHub REST API directly with curl
        curl -X POST \
          -H "Authorization: token ${{ github.token }}" \
          -H "Accept: application/vnd.github.v3+json" \
          https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments \
          -d "{\"body\":\"# AI Analysis of Test Failures\n\n$FEEDBACK\"}"