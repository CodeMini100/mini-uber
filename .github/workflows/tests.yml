name: Python Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 2  # Fetch at least two commits for diff
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f tests/requirements.txt ]; then pip install -r tests/requirements.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest pytest-cov httpx fastapi
        
    - name: Run tests and capture results
      id: test-run
      continue-on-error: true
      run: |
        mkdir -p test_artifacts
        python -m pytest -v > test_artifacts/test_results.txt 2>&1
        echo "status=$?" >> $GITHUB_ENV

    - name: Generate coverage report
      run: |
        pytest --cov=./ --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

    - name: Collect repository files
      if: env.status != '0'
      run: |
        mkdir -p test_artifacts/repo_files
        
        # Create a function to collect Python files
        collect_dir_files() {
          local dir=$1
          
          # Skip if directory doesn't exist
          if [ ! -d "$dir" ]; then
            echo "Directory $dir does not exist, skipping"
            return
          fi
          
          echo "Collecting files from $dir"
          find "$dir" -name "*.py" -type f -not -path "*/\.*" -not -path "*/__pycache__/*" | while read file; do
            dest_file="test_artifacts/repo_files/$file"
            mkdir -p "$(dirname "$dest_file")"
            cp "$file" "$dest_file"
          done
        }
        
        # Collect files from each directory
        collect_dir_files "payments"
        collect_dir_files "ratings"
        collect_dir_files "riders"
        collect_dir_files "rides"
        collect_dir_files "drivers"
        
        # Copy individual files
        cp main.py test_artifacts/repo_files/main.py
        cp config.py test_artifacts/repo_files/config.py
    
    - name: Prepare payload for API
      if: env.status != '0'
      run: |
        # Encode test results as base64 to preserve formatting
        TEST_RESULTS_BASE64=$(cat test_artifacts/test_results.txt | base64 -w 0)
        
        # Create initial JSON payload with test results only
        cat > test_artifacts/payload.json << EOF
        {
          "test_results_base64": "${TEST_RESULTS_BASE64}",
          "repo_files": {}
        }
        EOF
        
        # Add all repository files to payload
        echo "Adding repository files to payload"
        jq '.repo_files = {}' test_artifacts/payload.json > test_artifacts/payload_with_files.json
        
        find test_artifacts/repo_files -type f -name "*.py" | while read file; do
          # Get relative path
          rel_path=$(echo "$file" | sed 's|test_artifacts/repo_files/||')
          
          # Add file content to the payload
          FILE_CONTENT=$(cat "$file" | jq -Rs .)
          jq --arg file "$rel_path" --arg content "$FILE_CONTENT" '.repo_files[$file] = $content' test_artifacts/payload_with_files.json > test_artifacts/payload_tmp.json
          mv test_artifacts/payload_tmp.json test_artifacts/payload_with_files.json
        done
        
        mv test_artifacts/payload_with_files.json test_artifacts/payload.json
    
    - name: Request AI feedback if tests failed
      if: env.status != '0'
      run: |
        # Send to Vercel function
        curl -X POST https://uber-feedback-agent.vercel.app/api/analyze \
          -H "Content-Type: application/json" \
          -d @test_artifacts/payload.json \
          > test_artifacts/ai_feedback.json
        
        # Extract feedback from response
        cat test_artifacts/ai_feedback.json | jq -r '.feedback' > test_artifacts/ai_feedback.md
    
    - name: Upload AI feedback as artifact
      if: env.status != '0'
      uses: actions/upload-artifact@v2
      with:
        name: ai-feedback
        path: test_artifacts/ai_feedback.md
        
    - name: Comment on PR with feedback
      if: env.status != '0' && github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const feedback = fs.readFileSync('test_artifacts/ai_feedback.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `# AI Analysis of Test Failures\n\n${feedback}`
          });